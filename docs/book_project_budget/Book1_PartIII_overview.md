---
hide:
  - toc
---

# Part III: Deployment on Free-Tier Services

> *“Shipping your project is more than a final step—it’s the bridge between your code and the world.”*

---

Part III shows you how to **deploy your AI/ML apps without paying a cent.** Whether you’re launching a backend API or a frontend interface, this section walks you through the most reliable **free-tier cloud platforms**—Hugging Face, Railway, Render, Vercel, and Netlify.

You’ll learn which tool to use and when, how to manage environment variables, and how to keep your app running with minimal downtime and zero cost.

✅ Chapter 7: Backend Deployment Options  

- Explore the three major platforms for hosting AI inference backends:

* **Hugging Face Spaces** (for Gradio/FastAPI apps),
* **Railway** (for Dockerized FastAPI projects), and
* **Render** (a great fallback when Railway limits are hit).
  We’ll walk through example setups, pricing tiers, and pros/cons.

✅ Chapter 8: Frontend Deployment Options  

- Learn how to deploy your React-based frontend with **Vercel** or **Netlify**, two of the most beginner-friendly and scalable platforms. You’ll configure environment variables, link to your GitHub repo, and push updates with a single commit.

✅ Chapter 9: Fullstack Integration Walkthrough  

- Connect your deployed frontend and backend like a pro. We’ll show you how to safely expose public API routes, use `.env` secrets during builds, and monitor logs, performance, and usage in production environments.

After Part III, You Will Be Able To:

* Choose the right free-tier platform for backend or frontend hosting
* Deploy Gradio, FastAPI, or Docker apps using Hugging Face, Railway, or Render
* Host your frontend using Vercel or Netlify with CI/CD from GitHub
* Connect fullstack projects with secure public APIs
* Monitor and maintain your AI app after launch

---

> *In Part III, you become a deployer. No longer stuck in the notebook—you’ve now launched something real into the world.*
